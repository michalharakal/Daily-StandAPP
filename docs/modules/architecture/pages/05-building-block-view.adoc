= 5. Building Block View
:toc: left
:toclevels: 3
:icons: font

== 5.1 System Overview (Level 1)

[mermaid]
----
graph TB
    subgraph "DailyStandApp System"
        subgraph "Presentation"
            CLI[StandAPP-cli]
            MCPS[mcp-server]
            BENCH[benchmark]
        end

        subgraph "Core Modules"
            LLM[llm]
            DATA[data]
            DOMAIN[domain]
        end
    end

    CLI --> LLM
    CLI --> DATA
    MCPS --> LLM
    MCPS --> DATA
    MCPS --> DOMAIN
    BENCH --> LLM

    LLM --> DOMAIN
    DATA --> DOMAIN

    style CLI fill:#bbdefb
    style MCPS fill:#bbdefb
    style BENCH fill:#bbdefb
    style LLM fill:#c8e6c9
    style DATA fill:#c8e6c9
    style DOMAIN fill:#fff9c4
----

=== Module Overview

[cols="1,2,3"]
|===
|Module |Responsibility |Key Technologies

|`StandAPP-cli`
|Command-line interface
|JVM, Args parsing

|`mcp-server`
|MCP protocol server for AI assistants
|MCP Kotlin SDK, Ktor, Stdio transport

|`llm`
|LLM inference abstraction with pluggable backends
|SKaiNET kllama, REST API (Ktor Client), JLama, Coroutines

|`data`
|Git repository access
|Eclipse JGit

|`domain`
|Shared domain models
|Kotlin data classes

|`benchmark`
|LLM backend evaluation and comparison
|JVM, JSON test cases, Markdown/CSV reporting
|===

== 5.2 Module: llm (Level 2)

The LLM module provides AI inference capabilities with a pluggable architecture. Three backends are available, selected via the `MCP_LLM_BACKEND` environment variable.

[mermaid]
----
graph TB
    subgraph "llm module"
        subgraph "commonMain"
            IFACE[LLMSummarizer<br/>interface]
            EXPECT[expect fun<br/>getLLMSummarizer]
        end

        subgraph "jvmMain"
            ACTUAL[actual fun<br/>getLLMSummarizer]
            FACTORY[LLMServiceFactory]
            BACKEND[LLMBackendType<br/>enum]
            LCONFIG[LLMConfig]
            CONFIG[LLMEngineConfig]
            MOCK[MockJvmLLMSummarizer]
            SERVICE[LLMService<br/>interface - pluggable]
            REST[RestApiLLMService]
            JLAMA[JLamaService]
        end

        subgraph "jvmMain/kotlin-skainet"
            SKAINET[SkainetKLlamaService]
            SKSUM[SkainetLLMSummarizer]
            SKLLM[SKaiNetLLMService]
        end
    end

    EXPECT --> ACTUAL
    ACTUAL --> CONFIG
    CONFIG --> SKSUM
    ACTUAL -->|"test"| MOCK

    SKSUM --> SKAINET
    FACTORY --> BACKEND
    FACTORY --> LCONFIG
    FACTORY --> SKLLM
    FACTORY --> REST
    FACTORY --> JLAMA

    SKLLM --> SERVICE
    REST --> SERVICE
    JLAMA --> SERVICE

    style IFACE fill:#e1f5fe
    style SERVICE fill:#fff9c4
    style FACTORY fill:#e8f5e9
    style SKLLM fill:#fff3e0
    style REST fill:#fff3e0
    style JLAMA fill:#fff3e0
    style SKSUM fill:#c8e6c9
----

=== Key Classes

[cols="1,3"]
|===
|Class |Description

|`LLMSummarizer`
|Common interface: `summarize(text): String`, `summarizeStream(text): Flow<String>`

|`LLMService`
|JVM interface for pluggable backends: `generate(prompt, maxTokens, temperature, topP): String`

|`LLMServiceFactory`
|Factory that instantiates the right `LLMService` based on `MCP_LLM_BACKEND` environment variable or explicit `LLMConfig`

|`LLMBackendType`
|Enum: `SKAINET`, `REST_API`, `JLAMA` - selectable via `MCP_LLM_BACKEND` env var

|`LLMConfig`
|Data class for programmatic backend configuration (modelPath, baseUrl, modelName, apiKey)

|`SKaiNetLLMService`
|SKaiNET kllama implementation with GGUF model loading, streaming mode, chat templates

|`RestApiLLMService`
|OpenAI-compatible REST endpoint client (works with Ollama, llama.cpp server, LM Studio, vLLM)

|`JLamaService`
|Pure Java LLM inference backend

|`SkainetKLlamaService`
|Lower-level SKaiNET kllama wrapper with Mutex-based thread safety

|`SkainetLLMSummarizer`
|Production summarizer wrapping SkainetKLlamaService (used via `getLLMSummarizer()`)

|`LLMEngineConfig`
|Configuration via system properties (`llm.model`, `llm.max.seq.len`) for `getLLMSummarizer()`
|===

NOTE: New backends can be added by implementing the `LLMService` interface and registering them in `LLMServiceFactory`.

== 5.3 Module: data (Level 2)

The data module provides git repository access using expect/actual pattern.

[mermaid]
----
graph TB
    subgraph "data module"
        subgraph "commonMain"
            GITCLIENT[GitClient.kt<br/>expect functions]
            GITINFO[GitInfo<br/>data class]
        end

        subgraph "jvmMain"
            JVMIMPL[GitClient.jvm.kt<br/>actual functions]
            JGIT[Eclipse JGit]
        end

        subgraph "Other Platforms"
            STUBS[Stub implementations<br/>return emptyList]
        end
    end

    GITCLIENT --> JVMIMPL
    GITCLIENT --> STUBS
    JVMIMPL --> JGIT
    JVMIMPL --> GITINFO

    style GITINFO fill:#fff9c4
    style JVMIMPL fill:#c8e6c9
    style STUBS fill:#ffcdd2
----

=== Functions

[source,kotlin]
----
// commonMain - expect declarations
expect fun commitsByAuthorAndPeriod(
    repoDir: String,
    author: String,
    start: Instant,
    end: Instant
): List<GitInfo>

expect fun getAllCommitsInPeriod(
    repoDir: String,
    start: Instant,
    end: Instant
): List<GitInfo>

// Domain model
data class GitInfo(
    val id: String,
    val authorName: String,
    val authorEmail: String,
    val whenDate: Instant,
    val message: String
)
----

== 5.4 Module: mcp-server (Level 2)

The MCP server exposes git functionality to AI assistants via the Model Context Protocol.

[mermaid]
----
graph TB
    subgraph "mcp-server module"
        subgraph "Entry Point"
            MAIN[MCPServer.kt<br/>main]
            SETUP[standapp-server.kt<br/>run mcp server]
        end

        subgraph "Tools"
            TOOL1[get_commits_by_author]
            TOOL2[get_all_commits]
            SCHEMA[SchemaUtils]
        end

        subgraph "Transport"
            STDIO[StdioServerTransport]
            MCP[MCP Server SDK]
        end
    end

    subgraph "Dependencies"
        DATA[data module]
        LLM[llm module]
    end

    MAIN --> SETUP
    SETUP --> TOOL1
    SETUP --> TOOL2
    TOOL1 --> DATA
    TOOL2 --> DATA
    TOOL1 --> SCHEMA
    TOOL2 --> SCHEMA
    SETUP --> STDIO
    STDIO --> MCP

    style TOOL1 fill:#c8e6c9
    style TOOL2 fill:#c8e6c9
----

=== MCP Tools

[cols="1,2,3"]
|===
|Tool |Parameters |Description

|`get_commits_by_author`
|repoDir, author, startDate, endDate
|Fetch commits by specific author within date range

|`get_all_commits`
|repoDir, startDate, endDate
|Fetch all commits within date range
|===

== 5.5 Module: benchmark (Level 2)

The benchmark module evaluates and compares LLM backends using predefined test cases.

[mermaid]
----
graph TB
    subgraph "benchmark module"
        subgraph "Entry"
            MAIN[Main.kt<br/>CLI entry]
        end

        subgraph "Core"
            RUNNER[BenchmarkRunner]
            CASES[BenchmarkCase]
            PROMPTS[PromptTemplates]
            FORMAT[CommitFormatter]
        end

        subgraph "Evaluation"
            SCORING[Scoring]
            METRICS[Metrics]
        end

        subgraph "Output"
            REPORT[Reporting<br/>Markdown + CSV]
        end
    end

    subgraph "Dependencies"
        LLM[llm module<br/>LLMServiceFactory]
    end

    subgraph "Test Data"
        JSON[(bench/<br/>case-*.json)]
    end

    MAIN --> RUNNER
    RUNNER --> CASES
    RUNNER --> PROMPTS
    RUNNER --> FORMAT
    RUNNER --> LLM
    CASES --> JSON
    RUNNER --> SCORING
    RUNNER --> METRICS
    SCORING --> REPORT
    METRICS --> REPORT

    style RUNNER fill:#c8e6c9
    style SCORING fill:#fff9c4
    style REPORT fill:#e3f2fd
----

=== Benchmark Features

[cols="1,3"]
|===
|Feature |Description

|*Test Cases*
|15 JSON scenarios (normal, edge, stress) in `bench/` directory

|*Multi-Backend*
|Compares SKaiNET, JLama, and REST API backends side-by-side

|*Quality Scoring*
|JSON validity, schema compliance, hallucination detection

|*Performance Metrics*
|Latency (p50/p95), throughput, determinism across runs

|*Reporting*
|Generates Markdown reports and CSV results for analysis
|===

== 5.6 Component Interaction

[mermaid]
----
flowchart LR
    subgraph "External"
        CLIENT[MCP Client]
    end

    subgraph "mcp-server"
        TRANSPORT[Stdio Transport]
        SERVER[MCP Server]
        TOOLS[Tool Handlers]
    end

    subgraph "data"
        GITCLIENT[GitClient]
    end

    subgraph "llm"
        SUMMARIZER[LLMSummarizer]
    end

    CLIENT <-->|"JSON-RPC"| TRANSPORT
    TRANSPORT <--> SERVER
    SERVER --> TOOLS
    TOOLS --> GITCLIENT
    TOOLS --> SUMMARIZER
----
