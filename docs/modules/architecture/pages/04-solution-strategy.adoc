= 4. Solution Strategy
:toc: left
:toclevels: 3
:icons: font

== 4.1 Technology Decisions

[cols="1,2,3"]
|===
|Decision |Choice |Rationale

|*Language*
|Kotlin 2.2+ Multiplatform
|Type-safe, concise, cross-platform support with expect/actual pattern

|*Build System*
|Gradle with Kotlin DSL
|First-class Kotlin Multiplatform support, version catalogs

|*Git Library*
|Eclipse JGit
|Pure Java, no native dependencies, mature and stable

|*LLM Engine*
|SKaiNET kllama
|Pure Kotlin, no Python/native deps, SIMD via Vector API

|*Protocol*
|Model Context Protocol (MCP)
|Standard for AI assistant integration, supported by Claude

|*Model Format*
|GGUF
|Efficient quantization, widely supported, good performance
|===

== 4.2 Architecture Approach

=== Layered Architecture

[mermaid]
----
graph TB
    subgraph "Presentation Layer"
        CLI[StandAPP-cli]
        MCP[mcp-server]
        BENCH[benchmark]
    end

    subgraph "Application Layer"
        LLM[llm module]
        DATA[data module]
    end

    subgraph "Domain Layer"
        DOMAIN[domain module]
    end

    CLI --> LLM
    CLI --> DATA
    MCP --> LLM
    MCP --> DATA
    BENCH --> LLM

    LLM --> DOMAIN
    DATA --> DOMAIN

    style CLI fill:#bbdefb
    style MCP fill:#bbdefb
    style BENCH fill:#bbdefb
    style LLM fill:#c8e6c9
    style DATA fill:#c8e6c9
    style DOMAIN fill:#fff9c4
----

=== Multiplatform Strategy

[source,kotlin]
----
// Common interface (commonMain)
interface LLMSummarizer {
    suspend fun summarize(text: String): String
}

expect fun getLLMSummarizer(): LLMSummarizer

// JVM implementation (jvmMain)
actual fun getLLMSummarizer(): LLMSummarizer = when {
    System.getProperty("test.mode") == "true" -> MockJvmLLMSummarizer()
    else -> SkainetLLMSummarizer(SkainetKLlamaService.create(config))
}

// Other platforms return stubs or mock implementations
----

== 4.3 Key Design Decisions

=== Decision 1: Privacy-First Local AI (Non-Negotiable)

*Problem*: Git commits contain highly sensitive information - code changes, project names, business logic, author identities. Sending this data to cloud AI services is unacceptable for many organizations and individuals.

*Solution*: Embedded LLM inference using SKaiNET kllama, running entirely on the local machine. **No cloud AI services. No exceptions.**

*Why this matters*:
* Commit messages may reference security vulnerabilities, customer names, internal projects
* Code diffs reveal proprietary algorithms and business logic
* Author information is personally identifiable data (PII)
* Many enterprises prohibit sending code to external services

*Consequences*:
* (+) **Complete privacy** - zero data leaves the machine
* (+) Works offline after initial setup
* (+) Compliant with strict enterprise security policies
* (-) Requires JDK 21+ for Vector API
* (-) One-time model download required (~4GB for 7B models)

=== Decision 2: Pluggable LLM Architecture with SKaiNET kllama

*Problem*: Need local LLM inference without cloud dependencies, with flexibility to swap implementations.

*Solution*: Define `LLMService` interface for pluggable backends. Currently implemented with three backends, selectable via `MCP_LLM_BACKEND` environment variable:

* **SKaiNET kllama** - Pure Kotlin LLM with SIMD acceleration via Java Vector API
* **REST API** - OpenAI-compatible HTTP endpoint (Ollama, llama.cpp server, LM Studio, etc.)
* **JLama** - Pure Java LLM inference

[mermaid]
----
classDiagram
    class LLMService {
        <<interface>>
        +generate(prompt, maxTokens, temp, topP) String
    }

    class SKaiNetLLMService {
        -runtime: LlamaRuntime
        -tokenizer: GGUFTokenizer
        +generate()
    }

    class RestApiLLMService {
        -baseUrl: String
        -modelName: String
        -apiKey: String
        +generate()
    }

    class JLamaService {
        +generate()
    }

    class LLMServiceFactory {
        +create() LLMService
        +create(backendType, config) LLMService
    }

    LLMService <|.. SKaiNetLLMService
    LLMService <|.. RestApiLLMService
    LLMService <|.. JLamaService
    LLMServiceFactory ..> LLMService : creates
----

*Consequences*:
* (+) Pluggable architecture - easy to add new backends
* (+) Pure Kotlin with SKaiNET - no native library management
* (+) Same language as the rest of the project
* (+) Works on any JDK 21+ platform with Vector API
* (-) Requires JDK 21+ for optimal performance

=== Decision 3: MCP Protocol for AI Integration

*Problem*: Need to integrate with AI assistants like Claude.

*Solution*: Implement MCP server with stdio transport, exposing git tools.

*Benefits*:
* Standard protocol supported by major AI assistants
* Structured tool definitions with JSON schemas
* Bidirectional communication

== 4.4 Quality Strategy

[cols="1,3"]
|===
|Quality Goal |Strategy

|*Privacy*
|Local execution only, no network calls for AI features

|*Performance*
|SIMD acceleration via Java Vector API, KV-cache for inference

|*Portability*
|Kotlin Multiplatform with expect/actual for platform-specific code

|*Maintainability*
|Clean module boundaries, dependency injection via factory functions
|===
