= 8. Crosscutting Concepts
:toc: left
:toclevels: 3
:icons: font

== 8.1 Kotlin Multiplatform Architecture

=== Expect/Actual Pattern

The project uses Kotlin's expect/actual mechanism for platform-specific implementations.

[mermaid]
----
graph TB
    subgraph "commonMain"
        EXPECT[expect fun<br/>Platform-agnostic API]
    end

    subgraph "jvmMain"
        ACTUAL_JVM[actual fun<br/>JVM Implementation]
    end

    subgraph "nativeMain"
        ACTUAL_NATIVE[actual fun<br/>Native Stub]
    end

    subgraph "wasmJsMain"
        ACTUAL_WASM[actual fun<br/>Wasm Stub]
    end

    EXPECT --> ACTUAL_JVM
    EXPECT --> ACTUAL_NATIVE
    EXPECT --> ACTUAL_WASM

    style EXPECT fill:#e3f2fd
    style ACTUAL_JVM fill:#c8e6c9
    style ACTUAL_NATIVE fill:#ffcdd2
    style ACTUAL_WASM fill:#ffcdd2
----

=== Source Set Structure

[source,text]
----
module/src/
├── commonMain/kotlin/    # Shared interfaces and models
├── commonTest/kotlin/    # Shared tests
├── jvmMain/kotlin/       # JVM implementations
├── jvmMain/kotlin-skainet/  # SKaiNET-specific JVM code
├── nativeMain/kotlin/    # Native implementations (stubs)
├── androidMain/kotlin/   # Android implementations (stubs)
└── wasmJsMain/kotlin/    # WebAssembly implementations (stubs)
----

== 8.2 Dependency Injection

The project uses factory functions instead of a DI framework for simplicity. Two patterns coexist:

=== Pattern 1: Environment Variable Factory (MCP Server, Benchmark)

[source,kotlin]
----
// LLMServiceFactory selects backend from environment
object LLMServiceFactory {
    fun create(): LLMService {
        val backend = LLMBackendType.fromEnv() // MCP_LLM_BACKEND
        return when (backend) {
            LLMBackendType.SKAINET -> SKaiNetLLMService.create(modelPath)
            LLMBackendType.REST_API -> RestApiLLMService(baseUrl, model, apiKey)
            LLMBackendType.JLAMA -> JLamaService.create(modelPath, tokenizerPath)
        }
    }

    // Programmatic configuration (used by benchmark)
    fun create(backendType: LLMBackendType, config: LLMConfig): LLMService
}
----

=== Pattern 2: System Property Factory (getLLMSummarizer)

[source,kotlin]
----
// Conditional instantiation via system properties
actual fun getLLMSummarizer(): LLMSummarizer = when {
    System.getProperty("test.mode") == "true" ->
        MockJvmLLMSummarizer()
    else ->
        SkainetLLMSummarizer(SkainetKLlamaService.create(config))
}
----

== 8.3 Concurrency Model

[mermaid]
----
graph TB
    subgraph "Kotlin Coroutines"
        SUSPEND[suspend fun]
        FLOW[Flow<T>]
        MUTEX[Mutex]
    end

    subgraph "LLM Service"
        GENERATE["generate()"]
        STREAM["generateStream()"]
        LOCK[Thread Safety]
    end

    SUSPEND --> GENERATE
    FLOW --> STREAM
    MUTEX --> LOCK

    style SUSPEND fill:#e8f5e9
    style FLOW fill:#e8f5e9
    style MUTEX fill:#fff3e0
----

=== Thread Safety in SkainetKLlamaService

[source,kotlin]
----
class SkainetKLlamaService {
    private val mutex = Mutex()  // Coroutine-safe lock

    override suspend fun generate(...): String = mutex.withLock {
        runtime.reset()  // Reset state before each generation
        // ... inference logic
    }
}
----

== 8.4 Configuration Management

=== Environment Variables (Primary - MCP Server & Benchmark)

[cols="1,2,2"]
|===
|Variable |Default |Description

|`MCP_LLM_BACKEND`
|_(required)_
|Backend: `SKAINET`, `REST_API`, `JLAMA`

|`MCP_LLM_MODEL_PATH`
|_(required for SKAINET)_
|Path to GGUF model file

|`MCP_LLM_REST_BASE_URL`
|`http://localhost:11434`
|REST API endpoint URL

|`MCP_LLM_REST_MODEL`
|`llama3.2:3b`
|Model name for REST API

|`MCP_LLM_REST_API_KEY`
|_(optional)_
|Bearer token for authenticated endpoints
|===

=== System Properties (Legacy - getLLMSummarizer)

[cols="1,2,2"]
|===
|Property |Default |Description

|`llm.model`
|`./models/model.gguf`
|Path to GGUF model file

|`llm.temperature`
|`0.8`
|Sampling temperature for generation

|`llm.max.seq.len`
|`2048`
|Maximum sequence length for inference

|`test.mode`
|`false`
|Enable mock implementations for testing
|===

=== Configuration Flow

[mermaid]
----
flowchart LR
    ENV[Environment Variables]
    BACKEND[LLMBackendType.fromEnv]
    FACTORY[LLMServiceFactory]
    IMPL[LLMService Implementation]

    ENV --> BACKEND
    BACKEND --> FACTORY
    FACTORY --> IMPL
----

== 8.5 Error Handling

=== Strategy

[cols="1,2"]
|===
|Layer |Approach

|*Data Layer (Git)*
|Return empty list on error, log exception

|*LLM Layer*
|Propagate exceptions, let caller handle

|*MCP Server*
|Return error message in CallToolResult
|===

=== Example: Git Error Handling

[source,kotlin]
----
actual fun commitsByAuthorAndPeriod(...): List<GitInfo> {
    return try {
        val git = Git.open(File(repoDir))
        git.use { /* ... */ }
    } catch (e: Exception) {
        emptyList()  // Graceful degradation
    }
}
----

== 8.6 Logging and Observability

Currently minimal logging. Future considerations:

[cols="1,2"]
|===
|Aspect |Approach

|*Logging*
|SLF4J with simple backend (excluded from some modules)

|*Metrics*
|Not yet implemented

|*Tracing*
|MCP protocol provides request/response correlation
|===

== 8.7 Testing Strategy

[mermaid]
----
graph TB
    subgraph "Test Types"
        UNIT[Unit Tests]
        INTEGRATION[Integration Tests]
    end

    subgraph "Test Infrastructure"
        MOCK[MockJvmLLMSummarizer]
        KOTEST[Kotest Framework]
        COROUTINES[kotlinx-coroutines-test]
    end

    UNIT --> MOCK
    UNIT --> KOTEST
    INTEGRATION --> COROUTINES

    style MOCK fill:#fff3e0
    style KOTEST fill:#e8f5e9
----

=== Test Mode

[source,kotlin]
----
// Enable test mode via system property
tasks.withType<Test> {
    systemProperty("test.mode", "true")
}

// Mock implementation returns predictable results
class MockJvmLLMSummarizer : LLMSummarizer {
    override suspend fun summarize(text: String): String =
        "Mock summary of: ${text.take(50)}..."
}
----
