= 7. Deployment View
:toc: left
:toclevels: 3
:icons: font

== 7.1 Infrastructure Overview

[mermaid]
----
graph TB
    subgraph "Developer Machine"
        subgraph "JVM Runtime (JDK 21+)"
            MCP[mcp-server-jvm.jar]
            CLI[StandAPP-cli]
            BENCH[benchmark]
            CLOUDAPI[cloud-api:server<br/>port 8080]
        end

        subgraph "File System"
            REPOS[(Git Repositories)]
            MODELS[(GGUF Models<br/>~4GB)]
        end

        subgraph "AI Assistant"
            CLAUDE[Claude Desktop]
        end
    end

    CLAUDE -->|"stdio"| MCP
    MCP --> REPOS
    MCP --> MODELS
    CLI --> REPOS
    CLI --> MODELS
    BENCH --> MODELS

    style MCP fill:#c8e6c9
    style CLI fill:#bbdefb
    style BENCH fill:#bbdefb
    style CLOUDAPI fill:#e3f2fd
    style REPOS fill:#fff9c4
    style MODELS fill:#fce4ec
----

== 7.2 Deployment Artifacts

[cols="1,2,2,2"]
|===
|Artifact |Type |Build Command |Output

|`mcp-server-jvm.jar`
|Fat JAR
|`./gradlew :mcp-server:jvmJar`
|`mcp-server/build/libs/mcp-server-jvm.jar`

|CLI
|JAR
|`./gradlew :StandAPP-cli:jar`
|`StandAPP-cli/build/libs/*.jar`

|Benchmark
|Fat JAR
|`./gradlew :benchmark:jvmJar`
|`benchmark/build/libs/benchmark-jvm.jar`

|Cloud API Server
|Application (Gradle `run`)
|`./gradlew :cloud-api:server:run`
|Ktor server on port 8080

|Cloud API Client
|Application (Gradle `run`)
|`./gradlew :cloud-api:client:run`
|Connects to cloud-api server

|Cloud API Agent
|Application (Gradle `run`)
|`./gradlew :cloud-api:agent:run`
|Starts server + runs Koog agent

|Docker Image
|Container
|`docker compose build`
|MCP server in container with JDK 21
|===

== 7.3 MCP Server Deployment

=== Claude Desktop Configuration

Add to Claude Desktop's MCP configuration (`claude_desktop_config.json`):

[source,json]
----
{
  "mcpServers": {
    "standapp-git": {
      "command": "java",
      "args": [
        "--add-modules", "jdk.incubator.vector",
        "-Dllm.model=/path/to/model.gguf",
        "-jar",
        "/path/to/mcp-server-jvm.jar"
      ]
    }
  }
}
----

=== JVM Requirements

[mermaid]
----
graph LR
    subgraph "JVM Configuration"
        JDK[JDK 21+]
        VECTOR[--add-modules<br/>jdk.incubator.vector]
        PREVIEW[--enable-preview]
    end

    subgraph "System Properties"
        MODEL[-Dllm.model=...]
        SEQLEN[-Dllm.max.seq.len=2048]
        TEMP[-Dllm.temperature=0.8]
    end

    JDK --> VECTOR
    VECTOR --> PREVIEW

    style JDK fill:#e8f5e9
    style VECTOR fill:#fff3e0
----

== 7.4 Docker Deployment

A `Dockerfile` and `docker-compose.yml` are provided for containerized deployment.

[source,bash]
----
# Build and run with Docker Compose
docker compose build
docker compose up
----

The Docker image uses a multi-stage build with JDK 21 and packages the MCP server fat JAR.

== 7.5 Environment Variable Configuration

The application is primarily configured via environment variables:

[cols="1,2,2"]
|===
|Variable |Default |Description

|`MCP_LLM_BACKEND`
|_(required)_
|Backend selection: `SKAINET`, `REST_API`, or `JLAMA`

|`MCP_LLM_MODEL_PATH`
|_(required for SKAINET)_
|Path to GGUF model file

|`MCP_LLM_REST_BASE_URL`
|`http://localhost:11434`
|Base URL for REST API backend

|`MCP_LLM_REST_MODEL`
|`llama3.2:3b`
|Model name for REST API backend

|`MCP_LLM_REST_API_KEY`
|_(optional)_
|Bearer token for authenticated REST API endpoints
|===

== 7.6 Model Deployment

=== GGUF Model Requirements

[cols="1,2,3"]
|===
|Aspect |Requirement |Notes

|Format
|GGUF
|Quantized models (Q4_K_M, Q5_K_M recommended)

|Size
|~4GB for 7B models
|Smaller quantizations use less RAM

|Location
|Local file system
|Configure via `llm.skainet.model` property

|Compatibility
|LLaMA architecture
|Tested with Mistral, LLaMA 2/3
|===

=== Recommended Models

[source,bash]
----
# Download example (using huggingface-cli or manual download)
# Mistral 7B Instruct - Q4_K_M quantization
wget https://huggingface.co/.../mistral-7b-instruct-v0.3.Q4_K_M.gguf

# Set model path
java -Dllm.skainet.model=./models/mistral-7b-instruct-v0.3.Q4_K_M.gguf \
     --add-modules jdk.incubator.vector \
     -jar mcp-server-jvm.jar
----

== 7.7 Deployment Diagram

[mermaid]
----
graph TB
    subgraph WORKSTATION["Developer Workstation"]
        subgraph JVM["JVM 21+"]
            MCP_JAR[mcp-server-jvm.jar]
        end
        CLAUDE[Claude Desktop]
        GIT[(Git Repos)]
        MODELS[(GGUF Models)]
    end

    CLAUDE -->|stdio| MCP_JAR
    MCP_JAR -->|JGit| GIT
    MCP_JAR -->|SKaiNET| MODELS

    style MCP_JAR fill:#c8e6c9
    style CLAUDE fill:#e3f2fd
    style GIT fill:#fff9c4
    style MODELS fill:#fce4ec
----

== 7.8 Resource Requirements

[cols="1,2"]
|===
|Resource |Minimum Requirement

|*JDK*
|JDK 21 or later (for Vector API)

|*RAM*
|8GB minimum, 16GB recommended for 7B models

|*Disk*
|4-8GB for model files

|*CPU*
|Modern CPU with AVX2/AVX-512 for SIMD acceleration

|*OS*
|macOS, Linux, Windows (JVM supported platforms)
|===
