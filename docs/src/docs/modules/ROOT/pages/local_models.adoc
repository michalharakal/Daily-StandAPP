== ü§î JLama in cli

Add LLM Inference directly to your Java application.

== üî¨ Quick Start

=== üïµÔ∏è‚Äç‚ôÄÔ∏è How to use as a local client (with jbang!)

Jlama includes a command line tool that makes it easy to use.

The CLI can be run with https://www.jbang.dev/download/[jbang].

[source,shell]
----
#Install jbang (or https://www.jbang.dev/download/)
curl -Ls https://sh.jbang.dev | bash -s - app setup

#Install Jlama CLI (will ask if you trust the source)
jbang app install --force jlama@tjake
----

Now that you have jlama installed you can download a model from huggingface and chat with it.
Note I have pre-quantized models available at https://hf.co/tjake

[source,shell]
----
# Run the openai chat api and UI on a model
jlama restapi tjake/Llama-3.2-1B-Instruct-JQ4 --auto-download
----

open browser to http://localhost:8080/