== ü§î JLama in der Kommandozeile

Integriere LLM-Inferenz direkt in deine Java-Anwendung.

== üî¨ Schnellstart

=== üïµÔ∏è‚Äç‚ôÄÔ∏è Nutzung als lokaler Client (mit jbang!)

JLama enth√§lt ein Kommandozeilen-Tool, das die Nutzung sehr einfach macht.

Die CLI kann mit https://www.jbang.dev/download/[jbang] ausgef√ºhrt werden.

[source,shell]
----
# jbang installieren (oder https://www.jbang.dev/download/)
curl -Ls https://sh.jbang.dev | bash -s - app setup

# Jlama CLI installieren (fragt ggf., ob du der Quelle vertraust)
jbang app install --force jlama@tjake
----

NOTE: Wenn du Probleme hast, den Befehl `jlama` im Terminal auszuf√ºhren, stelle sicher, dass Jlama in `$HOME/.jbang/bin` installiert ist. Gegebenenfalls musst du die `PATH`-Variable erg√§nzen, damit sie auf den Ordner von Jbang verweist:

[source,shell]
----
export PATH="$PATH:$HOME/.jbang/bin"
----

Nachdem du Jlama installiert hast, kannst du ein Modell von Huggingface herunterladen und damit chatten. Vorgefertigte quantisierte Modelle findest du unter https://hf.co/tjake.

[source,shell]
----
# Starte die OpenAI Chat API und die UI mit einem Modell
jlama restapi tjake/TinyLlama-1.1B-Chat-v1.0-Jlama-Q4 --auto-download
----

√ñffne anschlie√üend deinen Browser unter http://localhost:8080/

WARNING: Wenn due ein Rechner mit Apple Silicon nutzt (z.B. Macbook Pro M1) stelle sicher, dass deine JVM nicht √ºber Rosseta l√§uft, aber direkt als ARM (z.B: ARM64 JDK).
https://github.com/tjake/Jlama/issues/68[Siehe Jlama issue f√ºr Details]