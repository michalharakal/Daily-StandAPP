<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Untitled :: daily-standapp</title>
    <link rel="canonical" href="https://michalharakal.github.io/Daily-StandAPP/daily-standapp/local_models.html">
    <meta name="generator" content="Antora 3.1.10">
    <link rel="stylesheet" href="../_/css/site.css">
    <script>var uiRootPath = '../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://michalharakal.github.io/Daily-StandAPP">daily-standapp</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="#">Home</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Products</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Product A</a>
            <a class="navbar-item" href="#">Product B</a>
            <a class="navbar-item" href="#">Product C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Services</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Service A</a>
            <a class="navbar-item" href="#">Service B</a>
            <a class="navbar-item" href="#">Service C</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">Resources</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="#">Resource A</a>
            <a class="navbar-item" href="#">Resource B</a>
            <a class="navbar-item" href="#">Resource C</a>
          </div>
        </div>
        <div class="navbar-item">
          <span class="control">
            <a class="button is-primary" href="#">Download</a>
          </span>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="daily-standapp" data-version="">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Dialy-StandAPP</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
    <button class="nav-item-toggle"></button>
    <span class="nav-text">Workshop</span>
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="setup.html">Set up an environment</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="bios.html">Wer wir sind</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="architecture.html">Architecture</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Dialy-StandAPP</span>
    <span class="version">default</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="index.html">Dialy-StandAPP</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">default</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
</nav>
  <div class="edit-this-page"><a href="https://github.com/michalharakal/Daily-StandAPP/docs/src/docs/modules/ROOT/pages/local_models.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<div class="paragraph">
<p><strong>Introduction:</strong> This guide will show you two beginner-friendly ways to download a large language model (LLM) to your local working folder. We cover: (1) using the Hugging Face Hub’s Command-Line Interface (CLI), and (2) using the <strong>Jlama</strong> project (a Java-based LLM engine). By the end, you’ll have the model files (weights, config, tokenizer, etc.) saved offline in a folder, ready for use. Let’s get started!</p>
</div>
<div class="sect1">
<h2 id="method-1-using-hugging-face-cli"><a class="anchor" href="#method-1-using-hugging-face-cli"></a>Method 1: Using Hugging Face CLI</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Hugging Face provides a CLI tool (<code>huggingface-cli</code>) as part of its <code>huggingface_hub</code> package. This lets you log in, search, and download models directly from the Hugging Face Hub via the terminal (<a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli#:~:text=The%20,and%20how%20to%20use%20them">Command Line Interface (CLI)</a>). Follow these steps to set it up and download a model locally:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Install the Hugging Face CLI tool</strong> – You can install it via Python’s package manager. For example, run:
 <code>bash
   pip install -U "huggingface_hub[cli]"
`<br>
 This installs the latest Hugging Face Hub client along with CLI support (<a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli#:~:text=%3E%3E%3E%20pip%20install%20">Command Line Interface (CLI)</a>). After installation, confirm it’s working by running `huggingface-cli --help</code> in your terminal.</p>
</li>
<li>
<p><strong>Log in to Hugging Face (if needed)</strong> – If you plan to download models that require authentication (e.g. large or gated models), log in first. Use the command:
 `bash
   huggingface-cli login
`<br>
 This will prompt you for an <strong>access token</strong> from your Hugging Face account (you can create one on the Hugging Face website under Settings). Paste your token and hit enter. A success message should appear once you’re logged in (<a href="https://huggingface.co/docs/huggingface_hub/main/en/guides/cli#:~:text=huggingface">Command Line Interface (CLI)</a>). (Public models don’t strictly require login, but it’s good to log in to avoid any access issues.)</p>
</li>
<li>
<p><strong>Search and select a model</strong> – Browse the Hugging Face model hub (visit <strong>huggingface.co</strong> in your browser) to find the model you want. You can use the website’s search to discover models (for example, search for “GPT-2” or “Llama 2”). Click the model you’re interested in and note the <strong>model repository name</strong> (usually in the format <code>"organization/model-name"</code> or <code>"username/model-name"</code>). For instance, the popular BERT base model has the repo name <code>bert-base-uncased</code>. Make sure you have this ID for the next step.</p>
</li>
<li>
<p><strong>Download the model files to your folder</strong> – Use the CLI’s download command with the model’s repo ID. For example, to download the <em>bert-base-uncased</em> model locally, run:
 <code>bash
   huggingface-cli download bert-base-uncased --local-dir .
`<br>
 Here, `--local-dir .</code> tells the CLI to save the files in the current directory (your working folder) instead of the default cache (<a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli#:~:text=The%20recommended%20,dir%60%20option">Command Line Interface (CLI)</a>). If you omit <code>--local-dir</code>, the files will be saved in Hugging Face’s cache directory on your system, and the CLI will output the path. Using the <code>download</code> command with just the repo name will fetch <strong>all files in the model repository</strong> (config, weights, tokenizer, etc.) (<a href="https://huggingface.co/docs/huggingface_hub/main/en/guides/cli#:~:text=Copied">Command Line Interface (CLI)</a>). You’ll see progress bars as it downloads each file.</p>
</li>
<li>
<p><strong>Verify the model files</strong> – After the command completes, check your working directory (or the folder you specified) to see the model files. You should see files such as the model’s weight file (e.g. <code>pytorch_model.bin</code> or <code>model.safetensors</code>), a <code>config.json</code>, and tokenizer files (such as <code>tokenizer.json</code>, <code>vocab.txt</code>, or merges files, depending on the model). For example, a Hugging Face model repo typically includes files like <strong>config.json</strong>, <strong>model.safetensors</strong>, <strong>tokenizer.json</strong>, and <strong>tokenizer_config.json</strong> (<a href="https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4/tree/main#:~:text=">tjake/Llama-3.2-1B-Instruct-JQ4 at main</a>). Make sure these are present. You can list the files in your folder (using <code>ls</code> on Unix or checking in your file explorer) to confirm all necessary files downloaded. If they’re present, congrats – you have the model offline!</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><em>(If anything is missing or you encounter an error, double-check that the model ID is correct and that you were logged in (for gated models). The CLI will print the local path of the downloaded files on success (<a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli#:~:text=%3E%3E%3E%20huggingface,gpt2%2Fsnapshots%2F11c5a3d5811f50298f278a704980280950aedb10%2Fconfig.json">Command Line Interface (CLI)</a>).)</em></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="method-2-using-jlama-java-llm-engine"><a class="anchor" href="#method-2-using-jlama-java-llm-engine"></a>Method 2: Using Jlama (Java LLM Engine)</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>What is Jlama?</strong> Jlama is a modern LLM inference engine written in Java (<a href="https://github.com/tjake/Jlama#:~:text=Jlama%20is%20a%20modern%20LLM,co%2Ftjake">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>). It allows you to download and run many Hugging Face text-generation models within a Java environment. In other words, it’s a tool that lets you work with LLMs locally, optimized with Java’s vector APIs for performance. According to the author, <em>“Jlama is a Java-based inference engine for many text-to-text models on HuggingFace… intended to be used for integrating AI into Java apps.”</em> (<a href="https://www.reddit.com/r/java/comments/1g8qwxt/jlama_llm_engine_for_java_20/#:~:text=Jlama%C2%A0is%20a%20java%20based%20inference,text%20models%20on%20huggingface">Jlama: LLM engine for Java 20+ : r/java</a>). Even if you’re not a Java developer, Jlama provides a simple CLI to fetch models, similar to the Hugging Face CLI.</p>
</div>
<div class="paragraph">
<p><strong>Setup and download steps:</strong> As a beginner, you can use Jlama’s CLI without writing any code. Ensure you have Java installed (Jlama requires a recent Java version – Java 21 is recommended for full support). Then follow these steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Install Jbang (prerequisite for Jlama CLI)</strong> – Jlama distributes its CLI via <a href="https://www.jbang.dev">Jbang</a>, which is a tool to run Java applications easily. First, install Jbang on your system. On Linux/Mac, you can run:
 `bash
   curl -Ls <a href="https://sh.jbang.dev" class="bare">sh.jbang.dev</a> | bash -s - app setup
`<br>
 (Windows users can find an installer on the Jbang website.) This script installs Jbang on your PATH (<a href="https://github.com/tjake/Jlama#:~:text=,app%20setup">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>). <em>If you already have Jbang, you can skip this step.</em></p>
</li>
<li>
<p><strong>Install the Jlama CLI</strong> – With Jbang ready, install Jlama by running:
 <code>bash
   jbang app install --force jlama@tjake
`<br>
 This command fetches the Jlama application from GitHub and installs the `jlama</code> CLI tool (you may be prompted to trust the source) (<a href="https://github.com/tjake/Jlama#:~:text=,app%20setup">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>). After this, you should have a <code>jlama</code> command available. You can test it by running <code>jlama version</code> or simply <code>jlama</code> to see the help message. The help should list various commands, including options to chat, run a REST API, download models, etc. (For reference, the Jlama CLI supports commands like <code>download</code> to get models, <code>list</code> to show local models, and others (<a href="https://github.com/tjake/Jlama#:~:text=Other%3A%20download%20%20%20,Display%20JLama%20version%20information">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>).)</p>
</li>
<li>
<p><strong>Download a model using Jlama</strong> – Now you can download a model from Hugging Face using Jlama. By default, Jlama will download a model and store it in a local directory for you. To do this, use the <code>jlama download</code> command with the model’s <strong>owner/name</strong> format (just like the Hugging Face repo ID). For example:
 `bash
   jlama download tjake/Llama-3.2-1B-Instruct-JQ4
`<br>
 This will download the model files for the specified repository (in this example, a 1B-parameter Llama 3.2 model quantized in 4-bit, hosted by the Jlama author). You can replace the repo ID with any model you want. Jlama will automatically handle downloading all required files from Hugging Face (<a href="https://github.com/tjake/Jlama#:~:text=Other%3A%20download%20%20%20,Display%20JLama%20version%20information">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>). During this process, you’ll see it fetching the model weights and other files.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p><strong>Note:</strong> You may want to run this command in an empty “models” folder or a dedicated working directory. Jlama will save the model files into a directory (often named after the model) in the current location. For instance, if you run the above command, it might create a folder like <code>Llama-3.2-1B-Instruct-JQ4/</code> containing the downloaded files. (The Jlama library uses the current working directory by default, or you can configure it programmatically to use a custom path (<a href="https://github.com/tjake/Jlama#:~:text=public%20void%20sample,JQ4%22%3B%20String%20workingDirectory%20%3D%20%22.%2Fmodels">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>), but running the CLI in your desired folder is the simplest approach.)</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Confirm the model files are downloaded</strong> – After <code>jlama download</code> finishes, verify that the model files are present. You can list the contents of the model’s folder to see files similar to what we discussed earlier (e.g. a <code>model.safetensors</code> file, <code>config.json</code>, <code>tokenizer.json</code>, etc., depending on the model). For example, the model we downloaded above would include those files (since Jlama focuses on safetensors format, you’ll see a <code>.safetensors</code> weights file). To double-check via Jlama, you can also run:
 <code>bash
   jlama list
`<br>
 This will output a list of models that Jlama has downloaded locally (<a href="https://github.com/tjake/Jlama#:~:text=Other%3A%20download%20%20%20,Display%20JLama%20version%20information">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>). Your newly downloaded model should appear in that list, confirming the download was successful. If it’s listed, you have the model saved on disk. (Jlama also provides a `jlama rm &lt;model&gt;</code> command to remove a model, and other utilities, as noted in the help output.)</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Now you have the LLM model files on your machine via both methods! 🎉 You can proceed to load these model files in your application or offline environment. The Hugging Face CLI method is convenient if you’re working in Python or any environment where Python is available, while Jlama is great for Java environments or if you want an all-in-one tool that also enables running the model (Jlama can even spin up a local API or chat interface for the model with commands like <code>jlama chat</code> or <code>jlama restapi</code>).</p>
</div>
<div class="paragraph">
<p><strong>Relevant Links:</strong> For more details, refer to the official Hugging Face documentation on the CL (<a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli#:~:text=The%20,and%20how%20to%20use%20them">Command Line Interface (CLI)</a>) (<a href="https://huggingface.co/docs/huggingface_hub/en/guides/cli#:~:text=The%20recommended%20,dir%60%20option">Command Line Interface (CLI)</a>)】 and the Jlama GitHub repository READM (<a href="https://github.com/tjake/Jlama#:~:text=%EF%B8%8F%E2%80%8D%E2%99%80%EF%B8%8F%20How%20to%20use%20as,with%20jbang">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>) (<a href="https://github.com/tjake/Jlama#:~:text=Other%3A%20download%20%20%20,Display%20JLama%20version%20information">GitHub - tjake/Jlama: Jlama is a modern LLM inference engine for Java</a>)】. Both sources provide additional usage examples and troubleshooting tips. With these tools, downloading and managing offline models becomes much easier – happy experimenting with your local LLMs!</p>
</div>
</div>
</div>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <p>This page was built using the Antora default UI.</p>
  <p>The source code for this UI is licensed under the terms of the MPL-2.0 license.</p>
</footer>
<script src="../_/js/site.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>
  </body>
</html>
